using MongoDB.Bson;
using MongoDB.Driver;
using OnlineMongoMigrationProcessor.Helpers;
using OnlineMongoMigrationProcessor.Models;
using OnlineMongoMigrationProcessor.Processors;
using System;
using System.Collections;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;

#pragma warning disable CS8618 // Non-nullable field must contain a non-null value when exiting constructor. Consider adding the 'required' modifier or declaring as nullable.
#pragma warning disable CS8629
#pragma warning disable CS8600
#pragma warning disable CS8602
#pragma warning disable CS8603
#pragma warning disable CS8604
#pragma warning disable CS8625
#pragma warning disable CS4014 // Because this call is not awaited, execution of the current method continues before the call is completed

namespace OnlineMongoMigrationProcessor
{


    public class MigrationWorker
    {
        public bool ProcessRunning { get; set; }

        private string _toolsDestinationFolder = $"{Helper.GetWorkingFolder()}mongo-tools";
        private string _toolsLaunchFolder = string.Empty;
        private bool _migrationCancelled = false;
        private JobList? _jobList;
        private MigrationJob? _job;
        private Log _log;
        private MongoClient? _sourceClient;
        private IMigrationProcessor _migrationProcessor;
        public MigrationSettings? _config;
        
        private ComparisonHelper? comparisonHelper;
        private CancellationTokenSource _compare_cts;

        public MigrationWorker(JobList jobList)
        {            
            _log = new Log();
            _jobList = jobList;
            jobList.SetLog(_log);
        }

        public LogBucket GetLogBucket(string jobId)
        {
            // only for active job in migration worker
            if (_job.Id == jobId)
                return _log.GetCurentLogBucket(jobId);
            else
                return null;
        }

        public List<LogObject> GetVerboseMessages(string jobId)
        {
            // only for active job in migration worker
            if (_job.Id == jobId)
                return _log.GetVerboseMessages();
            else
                return null;
        }

        public string GetRunningJobId()
        {
            if (_job != null)
            {
                if (_migrationProcessor != null && _migrationProcessor.ProcessRunning)
                {
                    return _job.Id;
                }
                else
                    return string.Empty;
            }
            else
            {
                return string.Empty;
            }
        }

        public bool IsProcessRunning(string id)
        {
            if (id != null && _job!=null && id == _job.Id)
            {
                if (_migrationProcessor != null)
                    return _migrationProcessor.ProcessRunning;
                else
                    return ProcessRunning;
            }
            else
            {
                return false;
            }
        }

        public void StopMigration()
        {
            try
            {
                _compare_cts?.Cancel();
                _jobList?.Save();
                _migrationCancelled = true;
                _migrationProcessor?.StopProcessing();
                ProcessRunning = false;
                _migrationProcessor = null;
            }
            catch { }
        }

        private async Task<TaskResult> PrepareForMigration()
        {
            _sourceClient = MongoClientFactory.Create(_log, _job.SourceConnectionString, false, _config.CACertContentsForSourceServer);
            _log.WriteLine("Source Client Created");
            if (_job.IsSimulatedRun)
            {
                _log.WriteLine("Simulated Run. No changes will be made to the target.");
            }
            else
            {
                if (_job.AppendMode)
                {
                    _log.WriteLine("Existing target collections will remain unchanged, and no indexes will be created.");
                }
                else
                {
                    if (_job.SkipIndexes)
                    {
                        _log.WriteLine("No indexes will be created.");
                    }
                }
            }


            if (_job.IsOnline)
            {
                _log.WriteLine("Checking if change stream is enabled on source");

                var retValue = await MongoHelper.IsChangeStreamEnabledAsync(_log, _config.CACertContentsForSourceServer, _job.SourceConnectionString, _job.MigrationUnits[0]);
                _job.SourceServerVersion = retValue.Version;
                _jobList?.Save();

                if (!retValue.IsCSEnabled)
                {
                    _job.IsCompleted = true;
                    StopMigration();
                    return TaskResult.Abort;
                }

            }

            _migrationProcessor?.StopProcessing(false);

            _migrationProcessor = null;
            switch (_job.JobType)
            {
                case JobType.MongoCopy:
                    _migrationProcessor = new CopyProcessor(_log, _jobList, _job, _sourceClient, _config);
                    break;
                case JobType.MongoDump:
                    _migrationProcessor = new DumpRestoreProcessor(_log, _jobList, _job, _sourceClient, _config, _toolsLaunchFolder);
                    break;
                case JobType.RUCopy:
                    _migrationProcessor = new RUCopyProcessor(_log, _jobList, _job, _sourceClient, _config);
                    break;
                default:
                    _log.WriteLine($"Unknown JobType: {_job.JobType}. Defaulting to MongoCopy.", LogType.Error);
                    _migrationProcessor = new CopyProcessor(_log, _jobList, _job, _sourceClient, _config);
                    break;
            }
            _migrationProcessor.ProcessRunning = true;

            return TaskResult.Success;
        }

        // Custom exception handler delegate with logic to control retry flow
        private async Task<TaskResult> Default_ExceptionHandler(Exception ex, int attemptCount, string processName, int currentBackoff)
        {

            _log.WriteLine($"{processName} attempt failed. Retrying in {currentBackoff} seconds...");
            return TaskResult.Retry;
        }

        // Custom exception handler delegate with logic to control retry flow
        private async Task<TaskResult> MigrateCollections_ExceptionHandler(Exception ex, int attemptCount, string processName, int currentBackoff)
        {

            if (ex is MongoExecutionTimeoutException)
            {
                _log.WriteLine($"{processName} attempt {attemptCount} failed due to timeout: {ex.ToString()}. Details:{ex.ToString()}", LogType.Error);
            }

            _log.WriteLine($"Retrying in {currentBackoff} seconds...", LogType.Error);
            return TaskResult.Retry;
        }

        private async Task<TaskResult> PreparePartitions()
        {
            bool checkedCS = false;
            foreach (var unit in _job.MigrationUnits)
            {
                if (_migrationCancelled) return TaskResult.Abort;

                if (await MongoHelper.CheckCollectionExists(_sourceClient, unit.DatabaseName, unit.CollectionName))
                {
                    unit.SourceStatus = CollectionStatus.OK;
                   

                    DateTime currrentTime= DateTime.UtcNow;

                    if (_job.IsOnline && unit.ResetChangeStream)
                    {
                        //if  reset CS needto get the latest CS resume token synchronously
                        _log.WriteLine($"Resetting Change Stream for {unit.DatabaseName}.{unit.CollectionName}. This can take upto 5 minutes");
                        await MongoHelper.SetChangeStreamResumeTokenAsync(_log, _sourceClient, _jobList, _job, unit);
                    }

                    if (unit.MigrationChunks == null || unit.MigrationChunks.Count == 0)
                    {

                        var chunks = await PartitionCollection(unit.DatabaseName, unit.CollectionName);

                        if (chunks.Count == 0)
                        {
                            _log.WriteLine($"{unit.DatabaseName}.{unit.CollectionName} has no records to migrate", LogType.Error);
                            unit.SourceStatus = CollectionStatus.NotFound;
                            continue;
                        }                     

                        if (!_job.IsSimulatedRun && !_job.AppendMode)
                        {
                            var database = _sourceClient.GetDatabase(unit.DatabaseName);
                            var collection = database.GetCollection<BsonDocument>(unit.CollectionName);
                            var result=await MongoHelper.DeleteAndCopyIndexesAsync(_log,unit, _job.TargetConnectionString, collection, _job.SkipIndexes);

                            if (!result)
                            {
                                return TaskResult.Retry;
                            }
                            _jobList?.Save();
                            if (_job.SyncBackEnabled && !_job.IsSimulatedRun && _job.IsOnline && !checkedCS)
                            {
                                _log.WriteLine("Sync Back: Checking if change stream is enabled on target");

                                var retValue = await MongoHelper.IsChangeStreamEnabledAsync(_log, string.Empty, _job.TargetConnectionString, unit, true);
                                checkedCS = true;
                                if (!retValue.IsCSEnabled)
                                {
                                    return TaskResult.Abort;
                                }
                            }                            

                        }

                        if (_job.IsOnline)
                        {
                            //run this job async to detect change stream resume token, if no chnage stream is detected, it will not be set and cancel in 5 minutes
                            Task.Run(async () =>
                            {
                                await MongoHelper.SetChangeStreamResumeTokenAsync(_log, _sourceClient, _jobList, _job, unit);
                            });

                        }
                        _log.WriteLine($"{unit.DatabaseName}.{unit.CollectionName} has {chunks.Count} chunk(s)");

                        unit.MigrationChunks = chunks;
                        unit.ChangeStreamStartedOn = currrentTime;

                    }

                }
                else
                {
                    unit.SourceStatus = CollectionStatus.NotFound;
                    _log.WriteLine($"{unit.DatabaseName}.{unit.CollectionName} does not exist on source or has zero records", LogType.Error);
                }
            }

            _jobList?.Save();
            return TaskResult.Success;
        }

        private async Task<TaskResult> MigrateJobCollections()
        {
            foreach (var migrationUnit in _job.MigrationUnits)
            {
                if (_migrationCancelled) break;

                if (migrationUnit.SourceStatus == CollectionStatus.OK)
                {
                    if (await MongoHelper.CheckCollectionExists(_sourceClient, migrationUnit.DatabaseName, migrationUnit.CollectionName))
                    {
                        MongoClient targetClient = null;
                        if (!_job.IsSimulatedRun)
                        {
                            targetClient = MongoClientFactory.Create(_log, _job.TargetConnectionString);

                            if (await MongoHelper.CheckCollectionExists(targetClient, migrationUnit.DatabaseName, migrationUnit.CollectionName))
                            {
                                if (!_job.CSPostProcessingStarted)
                                {
                                    _log.WriteLine($"{migrationUnit.DatabaseName}.{migrationUnit.CollectionName} already exists on the target.");
                                }
                            }
                        }
                        if (_migrationProcessor != null)
                        {
                            await _migrationProcessor.StartProcessAsync(migrationUnit, _job.SourceConnectionString, _job.TargetConnectionString);

                            // since CS processsing has started, we can break the loop. No need to process all collections
                            if (_job.IsOnline && _job.SyncBackEnabled && _job.CSPostProcessingStarted && Helper.IsOfflineJobCompleted(_job))
                            {
                                return TaskResult.Success;
                            }
                        }
                        else
                        {
                            migrationUnit.SourceStatus = CollectionStatus.NotFound;
                            _log.WriteLine($"{migrationUnit.DatabaseName}.{migrationUnit.CollectionName} does not exist on source or has zero records", LogType.Error);

                        }
                    }
                }                
            }
            return TaskResult.Success;
        }

        private void PopulateJobCollections(string namespacesToMigrate)
        {
            string[] collectionsInput = namespacesToMigrate
                .Split(',')
                .Select(item => item.Trim())
                .ToArray();

            if (_job.MigrationUnits.Count == 0)
            {
                foreach (var fullName in collectionsInput)
                {
                    if (_migrationCancelled) return;

                    int firstDotIndex = fullName.IndexOf('.');
                    if (firstDotIndex <= 0 || firstDotIndex == fullName.Length - 1) continue;

                    string dbName = fullName.Substring(0, firstDotIndex).Trim();
                    string colName = fullName.Substring(firstDotIndex + 1).Trim();

                    var migrationUnit = new MigrationUnit(dbName, colName, null);
                    _job.MigrationUnits.Add(migrationUnit);
                    _jobList?.Save();
                }
            }
        }

        public async Task StartMigrationAsync(MigrationJob job, string sourceConnectionString, string targetConnectionString, string namespacesToMigrate, bool doBulkCopy, bool trackChangeStreams)
        {
            _job = job;
            StopMigration(); //stop any existing
            ProcessRunning = true;


            //encoding speacial characters
            sourceConnectionString = Helper.EncodeMongoPasswordInConnectionString(sourceConnectionString);
            targetConnectionString = Helper.EncodeMongoPasswordInConnectionString(targetConnectionString);

            targetConnectionString = Helper.UpdateAppName(targetConnectionString, $"MSFTMongoWebMigration{_job.IsOnline}-" + job.Id);

            _job.TargetConnectionString = targetConnectionString;
            _job.SourceConnectionString = sourceConnectionString;

            LoadConfig();                      

            _migrationCancelled = false;


            string logfile=_log.Init(_job.Id);
            if (logfile != _job.Id)
            {
                _log.WriteLine($"Error in reading _log. Orginal log backed up as {logfile}");
            }
            _log.WriteLine($"{_job.Id} Started on {_job.StartedOn} (UTC)");
                        

            if (_job.MigrationUnits == null)
            {
                _job.MigrationUnits = new List<MigrationUnit>();
            }
            PopulateJobCollections(namespacesToMigrate);
                                 


            TaskResult result = await new RetryHelper().ExecuteTask(
                () => PrepareForMigration(),
                (ex, attemptCount, currentBackoff) => Default_ExceptionHandler(
                    ex, attemptCount,
                    "Preperation step",  currentBackoff
                ),
                _log
            );

            if (result == TaskResult.Abort || result == TaskResult.Failed || _migrationCancelled)
            {
                StopMigration();
                return;
            }

            result = await new RetryHelper().ExecuteTask(
                () => PreparePartitions(),
                (ex, attemptCount, currentBackoff) => Default_ExceptionHandler(
                    ex, attemptCount,
                    "Partition step", currentBackoff
                ),
                _log
            );

            if (result == TaskResult.Abort || result == TaskResult.Failed || _migrationCancelled)
            {
                StopMigration();
                return;
            }

            //if run comparison is set by customer.
            if (_job.RunComparison)
            { 
                var compareHelper = new ComparisonHelper();
                _compare_cts= new CancellationTokenSource();
                await compareHelper.CompareRandomDocumentsAsync(_log,_jobList,_job,_config, _compare_cts.Token);
                compareHelper = null;
                _job.RunComparison = false;
                _jobList?.Save();

                _log.WriteLine("Resuming migration.");
            }
            
            result = await new RetryHelper().ExecuteTask(
                () => MigrateJobCollections(),
                (ex, attemptCount, currentBackoff) => MigrateCollections_ExceptionHandler(
                    ex, attemptCount,
                    "Initiate migration", currentBackoff
                ),
                _log
            );

            if (result == TaskResult.Abort || result == TaskResult.Failed || _migrationCancelled)
            {
                StopMigration();
                return;
            }

        }

        private void LoadConfig()
        {
            if (_config == null)
                _config = new MigrationSettings();
             _config.Load();
        }


        public void SyncBackToSource(string sourceConnectionString, string targetConnectionString, MigrationJob job)
        {
            _job = job;

            ProcessRunning = true;

            LoadConfig();

            if(_log==null)
                _log = new Log();
            string logfile = _log.Init(_job.Id);

            _log.WriteLine($"Sync Back: {_job.Id} started on {_job.StartedOn} (UTC)");
            
            job.ProcessingSyncBack = true;
            _jobList.Save();

            if (_migrationProcessor != null)
                _migrationProcessor.StopProcessing();

            _migrationProcessor = null;
            _migrationProcessor = new SyncBackProcessor(_log,_jobList, _job, null, _config, string.Empty);
            _migrationProcessor.ProcessRunning = true;
            _migrationProcessor.StartProcessAsync(null, sourceConnectionString, targetConnectionString).GetAwaiter().GetResult();
            
        }

        private async Task<List<MigrationChunk>> PartitionCollection(string databaseName, string collectionName, string idField = "_id")
        {

            var stats = await MongoHelper.GetCollectionStatsAsync(_sourceClient, databaseName, collectionName);

            long documentCount = stats.DocumentCount;
            long totalCollectionSizeBytes = stats.CollectionSizeBytes;

            var database = _sourceClient.GetDatabase(databaseName);
            var collection = database.GetCollection<BsonDocument>(collectionName);

            int totalChunks = 0;
            long minDocsInChunk = 0;

            long targetChunkSizeBytes = _config.ChunkSizeInMb * 1024 * 1024;
            var totalChunksBySize = (int)Math.Ceiling((double)totalCollectionSizeBytes / targetChunkSizeBytes);


            if (_job.UseMongoDump)
            {
                totalChunks = totalChunksBySize;
                minDocsInChunk = documentCount / totalChunks;
                _log.WriteLine($"{databaseName}.{collectionName} Storage Size: {totalCollectionSizeBytes}");
            }
            else
            {
                _log.WriteLine($"{databaseName}.{collectionName} Estimated Document Count: {documentCount}");
                totalChunks = (int)Math.Min(SamplePartitioner.MaxSamples / SamplePartitioner.MaxSegments, documentCount / SamplePartitioner.MaxSamples);
                totalChunks = Math.Max(1, totalChunks); // At least one chunk
                totalChunks = Math.Max(totalChunks, totalChunksBySize);
                minDocsInChunk = documentCount / totalChunks;
            }

            List<MigrationChunk> migrationChunks = new List<MigrationChunk>();

            if (totalChunks > 1 || !_job.UseMongoDump)
            {
                _log.WriteLine($"Chunking {databaseName}.{collectionName}");                

                List<DataType> dataTypes = new List<DataType> { DataType.Int, DataType.Int64, DataType.String, DataType.Object, DataType.Decimal128, DataType.Date, DataType.ObjectId };

                if (_config.ReadBinary)
                {
                    dataTypes.Add(DataType.Binary);
                }

                foreach (var dataType in dataTypes)
                {
                    long docCountByType;
                    ChunkBoundaries chunkBoundaries = SamplePartitioner.CreatePartitions(_log,_job.UseMongoDump, collection, idField, totalChunks, dataType, minDocsInChunk, out docCountByType);

                    if (docCountByType == 0 || _job.UseMongoDump)   continue;

                    if (chunkBoundaries == null )
                    {
                       ProcessSegmentBoundaries(chunkBoundaries);                        
                    }
                    CreateSegments(chunkBoundaries, migrationChunks, dataType);
                }
            }
            else
            {
                var chunk = new MigrationChunk(string.Empty, string.Empty, DataType.String, false, false);
                migrationChunks.Add(chunk);
            }

            return migrationChunks;
        }

        private void ProcessSegmentBoundaries(ChunkBoundaries chunkBoundaries)
        {           
            var min = BsonNull.Value;
            var max = BsonNull.Value;

            var chunkBoundary = new Boundary
            {
                StartId = min,
                EndId = max,
                SegmentBoundaries = new List<Boundary>()
            };

            chunkBoundaries.Boundaries ??= new List<Boundary>();
            chunkBoundaries.Boundaries.Add(chunkBoundary);
            var segmentBoundary = new Boundary
            {
                StartId = min,
                EndId = max
            };
            chunkBoundary.SegmentBoundaries.Add(segmentBoundary);            
        }

        private void CreateSegments(ChunkBoundaries chunkBoundaries, List<MigrationChunk> migrationChunks, DataType dataType)
        {
            for (int i = 0; i < chunkBoundaries.Boundaries.Count; i++)
            {
                var (startId, endId) = GetStartEnd(true, chunkBoundaries.Boundaries[i], chunkBoundaries.Boundaries.Count, i);
                var chunk = new MigrationChunk(startId, endId, dataType, false, false);
                migrationChunks.Add(chunk);

                if (!_job.UseMongoDump && (chunkBoundaries.Boundaries[i].SegmentBoundaries == null || chunkBoundaries.Boundaries[i].SegmentBoundaries.Count == 0))
                {
                    chunk.Segments ??= new List<Segment>();
                    chunk.Segments.Add(new Segment { Gte = startId, Lt = endId, IsProcessed = false, Id = "1" });
                }

                if (!_job.UseMongoDump && chunkBoundaries.Boundaries[i].SegmentBoundaries.Count > 0)
                {
                    for (int j = 0; j < chunkBoundaries.Boundaries[i].SegmentBoundaries.Count; j++)
                    {
                        var segment = chunkBoundaries.Boundaries[i].SegmentBoundaries[j];
                        var (segmentStartId, segmentEndId) = GetStartEnd(false, segment, chunkBoundaries.Boundaries[i].SegmentBoundaries.Count, j, chunk.Lt, chunk.Gte);

                        chunk.Segments ??= new List<Segment>();
                        chunk.Segments.Add(new Segment { Gte = segmentStartId, Lt = segmentEndId, IsProcessed = false, Id = (j + 1).ToString() });
                    }
                }
            }
        }
        private Tuple<string, string> GetStartEnd(bool isChunk, Boundary boundary, int totalBoundaries, int currentIndex, string chunkLt = "", string chunkGte = "")
        {
            string startId;
            string endId;

            if (currentIndex == 0)
            {
                startId = isChunk ? "" : chunkGte;
                endId = boundary.EndId?.ToString() ?? "";
            }
            else if (currentIndex == totalBoundaries - 1)
            {
                startId = boundary.StartId?.ToString() ?? "";
                endId = isChunk ? "" : chunkLt;
            }
            else
            {
                startId = boundary.StartId?.ToString() ?? "";
                endId = boundary.EndId?.ToString() ?? "";
            }

            return Tuple.Create(startId, endId);
        }           
 
    }
}
